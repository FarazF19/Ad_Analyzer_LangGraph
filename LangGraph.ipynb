{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd914f93",
   "metadata": {},
   "source": [
    "## LangGraph End-to-End App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cbc5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file at: C:\\Users\\user\\Desktop\\LangGraph-app\\astra-bundle.zip\n",
      "File not found at: C:\\Users\\user\\Desktop\\LangGraph-app\\astra-bundle.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cassio\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SECURE_BUNDLE_PATH = Path(r\"C:\\Users\\user\\Desktop\\LangGraph-app\\astra-bundle.zip\")\n",
    "\n",
    "\n",
    "print(f\"Looking for file at: {SECURE_BUNDLE_PATH.absolute()}\")\n",
    "\n",
    "ASTRA_DB_APP_TOKEN=os.getenv(\"ASTRA_DB_APP_TOKEN\")\n",
    "ASTRA_DB_CLIENT_ID=os.getenv(\"ASTRA_DB_CLIENT_ID\")\n",
    "# Verify that the file exists before passing it to Cassio\n",
    "if os.path.exists(SECURE_BUNDLE_PATH):\n",
    "    print(\"File found, initializing Cassio...\")\n",
    "    \n",
    "    # Open the file in binary read mode if needed\n",
    "    with open(SECURE_BUNDLE_PATH, 'rb') as bundle_file:\n",
    "        # Pass the file handler to Cassio (Cassio should support this)\n",
    "        cassio.init(token=ASTRA_DB_APP_TOKEN, database_id=ASTRA_DB_CLIENT_ID, secure_connect_bundle=bundle_file)\n",
    "else:\n",
    "    print(f\"File not found at: {SECURE_BUNDLE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7113c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fb ads - mock function\n",
    "\n",
    "def get_facebook_ads():\n",
    "    \"\"\"\n",
    "    Mock function to simulate fetching Facebook ads.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of ad dictionaries with text and HTML (containing video link).\n",
    "    \"\"\"\n",
    "    ads = [\n",
    "        {\n",
    "            \"ad_text\": \"Huge discount on shoes! Watch our promo video.\",\n",
    "            \"html\": \"<div><a href='https://facebook.com/ads/video1'>Watch</a></div>\"\n",
    "        },\n",
    "        {\n",
    "            \"ad_text\": \"Try our smart gadgets today!\",\n",
    "            \"html\": \"<div><a href='https://facebook.com/ads/video2'>Demo</a></div>\"\n",
    "        }\n",
    "    ]\n",
    "    print(\"[Node 1] Retrieved Facebook ads.\")\n",
    "    return {\"ads\": ads}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d3a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_urls.py\n",
    "import re\n",
    "\n",
    "def extract_video_urls(state):\n",
    "    \"\"\"\n",
    "    Mock function to extract video URLs from Facebook ads HTML.\n",
    "\n",
    "    Args:\n",
    "        state (dict): LangGraph state containing 'ads' key.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'video_ads' containing ad text and video URLs.\n",
    "    \"\"\"\n",
    "    ads = state.get(\"ads\", [])\n",
    "    video_ads = []\n",
    "\n",
    "    for ad in ads:\n",
    "        html = ad.get(\"html\", \"\")\n",
    "        match = re.search(r\"https://facebook\\.com/ads/video\\d+\", html)\n",
    "        if match:\n",
    "            video_ads.append({\n",
    "                \"ad_text\": ad[\"ad_text\"],\n",
    "                \"video_url\": match.group(0)\n",
    "            })\n",
    "\n",
    "    print(f\"[Node 2] Extracted {len(video_ads)} video URLs.\")\n",
    "    return {\"video_ads\": video_ads}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f22c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(state):\n",
    "    \"\"\"\n",
    "    Mock function to 'download' video from URLs.\n",
    "    This just simulates download by creating a fake local path.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Contains 'video_ads' from previous node.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'video_paths'.\n",
    "    \"\"\"\n",
    "    video_ads = state.get(\"video_ads\", [])\n",
    "    video_paths = []\n",
    "\n",
    "    for i, ad in enumerate(video_ads):\n",
    "        # Pretend we're downloading and saving a video locally\n",
    "        fake_path = f\"/mock/path/video_{i}.mp4\"\n",
    "        video_paths.append({\n",
    "            \"ad_text\": ad[\"ad_text\"],\n",
    "            \"video_url\": ad[\"video_url\"],\n",
    "            \"local_path\": fake_path\n",
    "        })\n",
    "\n",
    "    print(f\"[Node 3] Mock downloaded {len(video_paths)} videos.\")\n",
    "    return {\"video_paths\": video_paths}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(state):\n",
    "    \"\"\"\n",
    "    Mock function to extract 5 frames from each video.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Contains 'video_paths'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'frames'.\n",
    "    \"\"\"\n",
    "    video_paths = state.get(\"video_paths\", [])\n",
    "    frames = []\n",
    "\n",
    "    for video in video_paths:\n",
    "        frame_list = [f\"{video['local_path']}_frame_{i}.jpg\" for i in range(5)]\n",
    "        frames.append({\n",
    "            \"ad_text\": video[\"ad_text\"],\n",
    "            \"video_url\": video[\"video_url\"],\n",
    "            \"frames\": frame_list\n",
    "        })\n",
    "\n",
    "    print(f\"[Node 4] Extracted 5 mock frames per video.\")\n",
    "    return {\"frames\": frames}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650411b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_video(state):\n",
    "    \"\"\"\n",
    "    Mock transcription using OpenAI's Whisper (simulated).\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Contains 'video_paths'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'transcriptions'.\n",
    "    \"\"\"\n",
    "    video_paths = state.get(\"video_paths\", [])\n",
    "    transcriptions = []\n",
    "\n",
    "    for video in video_paths:\n",
    "        # Simulate transcription text\n",
    "        fake_transcript = f\"Mock transcript for {video['local_path']}\"\n",
    "        transcriptions.append({\n",
    "            \"video_url\": video[\"video_url\"],\n",
    "            \"transcript\": fake_transcript\n",
    "        })\n",
    "\n",
    "    print(f\"[Node 5] Transcribed {len(transcriptions)} videos.\")\n",
    "    return {\"transcriptions\": transcriptions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c89f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_32\\1022398805.py:13: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Mock embedding function (will be replaced with OpenAIEmbeddings in production)\n",
    "class MockEmbeddingFunction:\n",
    "    def embed_documents(self, texts):\n",
    "        return [[0.1] * 1536 for _ in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return [0.1] * 1536\n",
    "\n",
    "# Setup vectorstore\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"video_knowledge_base\",\n",
    "    embedding_function=MockEmbeddingFunction()\n",
    ")\n",
    "\n",
    "# Node function to embed + store + return retriever\n",
    "def embed_and_store_video_data(state):\n",
    "    \"\"\"\n",
    "    Combines transcript and frame analysis into a single document,\n",
    "    embeds it, stores it in ChromaDB, and returns a retriever.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Must contain 'transcriptions' and 'frame_analysis'.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated state including a retriever.\n",
    "    \"\"\"\n",
    "    transcriptions = state.get(\"transcriptions\", [])\n",
    "    frame_analysis = state.get(\"frame_analysis\", [])\n",
    "\n",
    "    for i, transcript_obj in enumerate(transcriptions):\n",
    "        url = transcript_obj[\"video_url\"]\n",
    "        transcript = transcript_obj[\"transcript\"]\n",
    "\n",
    "        # Find corresponding frame analysis\n",
    "        analysis = next((item[\"results\"] for item in frame_analysis if item[\"video_url\"] == url), [])\n",
    "\n",
    "        # Combine text\n",
    "        combined_text = f\"Transcript:\\n{transcript}\\n\\nFrame Analysis:\\n\" + \"\\n\".join(analysis)\n",
    "\n",
    "        # Store as LangChain Document\n",
    "        doc = Document(\n",
    "            page_content=combined_text,\n",
    "            metadata={\"source\": url, \"type\": \"facebook_video_ad\"}\n",
    "        )\n",
    "\n",
    "        vectorstore.add_documents([doc])\n",
    "        print(f\"[Node 7] Embedded and stored video: {url}\")\n",
    "\n",
    "    # Create retriever and return\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return {\"retriever\": retriever}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20efa6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transcriptions(state):\n",
    "    \"\"\"\n",
    "    Mock function to analyze video transcriptions.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Should contain 'transcriptions' key.\n",
    "\n",
    "    Returns:\n",
    "        dict: Adds 'transcript_analysis' to state.\n",
    "    \"\"\"\n",
    "    transcriptions = state.get(\"transcriptions\", [])\n",
    "    transcript_analysis = []\n",
    "\n",
    "    for item in transcriptions:\n",
    "        video_url = item[\"video_url\"]\n",
    "        transcript = item[\"transcript\"]\n",
    "\n",
    "        # Mock NLP analysis output\n",
    "        analysis_result = {\n",
    "            \"video_url\": video_url,\n",
    "            \"insights\": {\n",
    "                \"detected_tone\": \"friendly\",\n",
    "                \"call_to_action\": \"Sign up now!\",\n",
    "                \"intent\": \"Promotional\"\n",
    "            }\n",
    "        }\n",
    "        transcript_analysis.append(analysis_result)\n",
    "\n",
    "    print(f\"[Node 8] Analyzed {len(transcript_analysis)} transcriptions.\")\n",
    "    return {\"transcript_analysis\": transcript_analysis}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fba7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frames(state):\n",
    "    \"\"\"\n",
    "    Mock visual frame analysis.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Contains 'frames'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'frame_analysis'.\n",
    "    \"\"\"\n",
    "    frame_batches = state.get(\"frames\", [])\n",
    "    analysis_results = []\n",
    "\n",
    "    for batch in frame_batches:\n",
    "        results = [f\"Analysis for {frame}\" for frame in batch[\"frames\"]]\n",
    "        analysis_results.append({\n",
    "            \"video_url\": batch[\"video_url\"],\n",
    "            \"results\": results\n",
    "        })\n",
    "\n",
    "    print(f\"[Node 6] Analyzed frames for {len(analysis_results)} videos.\")\n",
    "    return {\"frame_analysis\": analysis_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = 'your-api-key-here'\n",
    "\n",
    "def base64_to_binary(base64_data: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Converts base64-encoded image data to binary.\n",
    "    \n",
    "    Args:\n",
    "        base64_data (str): Base64-encoded image data.\n",
    "    \n",
    "    Returns:\n",
    "        bytes: Binary image data.\n",
    "    \"\"\"\n",
    "    # Decode the base64 string to binary data\n",
    "    binary_data = base64.b64decode(base64_data)\n",
    "    return binary_data\n",
    "\n",
    "def analyze_image(base64_image: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the image using OpenAI's Image model.\n",
    "    \n",
    "    Args:\n",
    "        base64_image (str): Base64-encoded image data.\n",
    "    \n",
    "    Returns:\n",
    "        str: The result from the image analysis (e.g., a description of the image).\n",
    "    \"\"\"\n",
    "    # Convert base64 image data to binary\n",
    "    binary_image_data = base64_to_binary(base64_image)\n",
    "\n",
    "    # Using OpenAI's Image API to analyze the image\n",
    "    response = openai.Image.create(\n",
    "        prompt=\"Give me a single word that represents a characteristic of this advertising image to characterize it.\",\n",
    "        n=1,\n",
    "        images=[{\n",
    "            \"file\": binary_image_data,\n",
    "            \"filename\": \"ad_image.jpg\"\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Extracting the insights\n",
    "    image_analysis_result = response.get(\"data\", [{}])[0].get(\"text\", \"\")\n",
    "    \n",
    "    return image_analysis_result\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our LangGraph agent.\n",
    "\n",
    "    Attributes:\n",
    "        question: Input query or task.\n",
    "        generation: Output or LLM-generated content.\n",
    "        documents: Supporting docs or evidence list.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "\n",
    "def analyze_ad(ad_transcript: str, ad_visual_summary: str, base64_image: str) -> GraphState:\n",
    "    \"\"\"\n",
    "    Analyzes ad insights using OpenAI model and returns the result as a GraphState.\n",
    "    \n",
    "    Args:\n",
    "        ad_transcript (str): Audio description or transcript summary.\n",
    "        ad_visual_summary (str): Video description or visual summary.\n",
    "        base64_image (str): Base64-encoded image data for visual analysis.\n",
    "    \n",
    "    Returns:\n",
    "        GraphState: A dictionary representing the analysis state.\n",
    "    \"\"\"\n",
    "    # Analyze the image for additional insights\n",
    "    image_analysis_result = analyze_image(base64_image)\n",
    "    \n",
    "    # Construct the prompt for OpenAI analysis based on both transcript, visual summary, and image analysis\n",
    "    prompt = f\"\"\"\n",
    "    STEP 1: Analyze the ad insights below.\n",
    "    Ad Transcript Summary: {ad_transcript}\n",
    "    Visual Summary: {ad_visual_summary}\n",
    "    Image Analysis: {image_analysis_result}\n",
    "    \n",
    "    Now answer these clearly:\n",
    "    1. What is the **main hook line or pattern** used in this ad? Why did it work?\n",
    "    2. What is the **tone** of the ad (e.g., emotional, confident, hype)?\n",
    "    3. What **power phrases or emotional angles** stood out?\n",
    "    4. What **gestures, expressions, or camera angles or visual things** were impactful?\n",
    "    \n",
    "    Please reply in the following JSON format:\n",
    "    {{\n",
    "        \"hook\": \"...\",\n",
    "        \"tone\": \"...\",\n",
    "        \"power_phrases\": \"...\",\n",
    "        \"visual\": \"...\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send the prompt to OpenAI model (GPT or similar)\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    # Extracting the insights from the response\n",
    "    analysis_result = response.choices[0].text.strip()\n",
    "    \n",
    "    # Creating the GraphState to represent the result\n",
    "    graph_state: GraphState = {\n",
    "        \"question\": \"What insights can we extract from this ad?\",\n",
    "        \"generation\": analysis_result,\n",
    "        \"documents\": [ad_transcript, ad_visual_summary, image_analysis_result]\n",
    "    }\n",
    "    \n",
    "    return graph_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e435f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e344bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c3ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf811c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab914fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
